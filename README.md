# ğŸ§  Local AI Assistant `(Unity Tool)`

![Unity Version](https://img.shields.io/badge/Unity-2021.3%2B-000000?style=flat-square&logo=unity)
![License](https://img.shields.io/badge/License-MIT-blue?style=flat-square)
![Platform](https://img.shields.io/badge/Platform-macOS%20%7C%20Windows-lightgrey?style=flat-square)
![Status](https://img.shields.io/badge/Status-Working%20Prototype-brightgreen?style=flat-square)

> **"Privacy-focused, offline AI coding companion living directly inside your Unity Editor."**

---

## ğŸ“– The Story

I built this tool because I wanted a smart assistant to help with my Unity projectsâ€”answering code questions, explaining exceptions, and brainstorming mechanicsâ€”**without** sending my proprietary code to the cloud or burning through API credits.

It runs a quantization-optimized mid-tier LLM (**Mistral 7B**) locally on your machine using `llama.cpp`. No internet required. No data leaks. Just you and your AI.

---

## âœ¨ Key Features

| Feature | Description |
| :--- | :--- |
| ğŸ›¡ï¸ **100% Private** | Your project data never leaves `localhost`. |
| ğŸ’¸ **Free Forever** | Runs on your hardware (CPU / Metal / CUDA). |
| ğŸ§© **Unity Native** | Built with **UI Toolkit** for a polished Editor look. |
| âš¡ **Context Aware** | Reads your selected GameObject or Console errors. |
| ğŸ”’ **Safe Interop** | No `unsafe` code â€” uses managed marshalling for stability. |

---

## ğŸš€ Quick Start

### 1. Install Native Libraries
> **Tools â†’ Local AI â†’ Install Native Libraries**

This downloads the pre-built `llama.cpp` binaries (~50MB) from the official GitHub release and places them in `Assets/LocalAI/Plugins/`.

### 2. Download the AI Model
> **Tools â†’ Local AI Assistant â†’ Download Model**

Downloads **Mistral 7B Instruct (Q4_K_M)** (~4GB) to:
- macOS: `~/Library/Application Support/LocalAIUnity/models/`
- Windows: `%AppData%\LocalAIUnity\models\`

### 3. Start Using
1.  Select a **GameObject** in your scene.
2.  Open **Tools â†’ Local AI Assistant**.
3.  Click **"Generate"** or **"Explain Code"**.
4.  Watch the AI stream its response in real-time!

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
| :--- | :--- |
| **UI** | Unity UI Toolkit (UXML / USS) |
| **Backend** | C# with `Marshal`-based safe P/Invoke |
| **Inference** | `llama.cpp` (b7423 release) |
| **Model** | [Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) |

---

## ï¿½ Project Structure

```
Assets/LocalAI/
â”œâ”€â”€ Editor/
â”‚   â”œâ”€â”€ Services/
â”‚   â”‚   â”œâ”€â”€ ModelManager.cs          # Model state & download
â”‚   â”‚   â”œâ”€â”€ ModelDownloadService.cs  # HTTP resume support
â”‚   â”‚   â”œâ”€â”€ InferenceService.cs      # Token generation loop
â”‚   â”‚   â””â”€â”€ ContextCollector.cs      # Editor context gathering
â”‚   â”œâ”€â”€ UI/
â”‚   â”‚   â”œâ”€â”€ LocalAIEditorWindow.cs   # Main window
â”‚   â”‚   â”œâ”€â”€ HeaderView.cs
â”‚   â”‚   â”œâ”€â”€ ContextView.cs
â”‚   â”‚   â”œâ”€â”€ ResponseView.cs
â”‚   â”‚   â””â”€â”€ ActionBarView.cs
â”‚   â””â”€â”€ Setup/
â”‚       â””â”€â”€ NativeSetup.cs           # Binary installer
â”œâ”€â”€ Runtime/
â”‚   â””â”€â”€ Native/
â”‚       â””â”€â”€ LLMNativeBridge.cs       # Safe P/Invoke bindings
â””â”€â”€ Plugins/
    â””â”€â”€ (libllama.dylib / llama.dll) # Downloaded automatically
```

---

## ğŸ”® Roadmap

- [ ] **Sampler Options**: Add temperature, top-k, top-p controls.
- [ ] **Chat History**: Persist context across sessions.
- [ ] **Real-time Log Monitoring**: Auto-detect console errors.
- [ ] **Agent Mode**: Let AI write files to your project (safely!).

---

## âš ï¸ Known Limitations

- **First Load is Slow**: The 4GB model takes ~10-30 seconds to load into memory on first use.
- **Memory Usage**: Requires ~6GB RAM during inference.
- **Struct Compatibility**: The native bridge assumes `llama.cpp` release `b7423`. Other versions may have different struct layouts.

---

## ğŸ“œ License

MIT â€” Use it, modify it, ship it. Just don't blame me if the AI suggests deleting `System32`.

---

<p align="center">
  <i>Built with â¤ï¸, C#, and way too much coffee.</i> â˜•
</p>
